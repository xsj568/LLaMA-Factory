### 概览

- **目标**: 说明 LLaMA-Factory 的微调（微淘/微调）整体逻辑：数据处理、模型加载、训练编排。
- **核心入口**: `src/train.py` → `llamafactory.train.tuner.run_exp` → 根据阶段分派到 `pt/sft/rm/ppo/dpo/kto` 各自 `workflow.py`。
- **关键参数**: 通过 `llamafactory.hparams.parser.get_train_args` 解析得到 `ModelArguments / DataArguments / TrainingArguments / FinetuningArguments / GeneratingArguments`。

### 参数与入口

- 入口函数：`src/train.py: main()` 调用 `run_exp()`。
- 参数解析：`src/llamafactory/hparams/parser.py`
  - `get_train_args()` 内部完成：
    - 校验阶段与互斥参数（如非 SFT 禁止 `predict_with_generate`、`neat_packing` 等）。
    - 要求分布式环境（`ParallelMode.NOT_DISTRIBUTED` 将报错）。
- 训练调度：`src/llamafactory/train/tuner.py` 的 `_training_function`：
  - 组装回调（日志、早停、SwanLab、Reporter 等）。
  - 按 `finetuning_args.stage` 调用：
    - `pt`: 预训练阶段
    - `sft`: 监督微调阶段
    - `rm`: 奖励模型
    - `ppo`: 强化学习 PPO
    - `dpo`: 直接偏好优化
    - `kto`: KTO 反馈优化

### 数据处理流程

- 统一入口：`src/llamafactory/data/loader.py:get_dataset(...)`
  - 若 `DataArguments.tokenized_path` 存在且可用，优先从磁盘加载已分词数据并返回对应模块。
  - 否则执行“加载→预处理→划分”流程：
    1. 合并多数据集：`_get_merged_dataset(...)` 支持 `dataset` 与 `eval_dataset`，可按 `eval_on_each_dataset` 返回 dict。
    2. 预处理：`_get_preprocessed_dataset(...)`
       - 选择数据处理器 `_get_dataset_processor(...)`：
         - `pt`: `PretrainDatasetProcessor`
         - `sft`: `SupervisedDatasetProcessor` 或 `PackedSupervisedDatasetProcessor`（`packing/neat_packing`）
         - `rm`: `PairwiseDatasetProcessor`
         - `kto`: `FeedbackDatasetProcessor`
       - 基于 `datasets.Dataset.map` 批量分词、格式校验、去除原始列。
       - `predict_with_generate` 时（评估）允许处理生成所需字段。
    3. 划分：`split_dataset(...)` 按需切分训练/验证；可保存分词后数据到 `tokenized_path`。

- Tokenizer 与模板：
  - `load_tokenizer(model_args)` 返回 tokenizer 及相关组件。
  - `get_template_and_fix_tokenizer(tokenizer, data_args)` 根据对话模板修正 tokenizer 特殊符号，确保模板与模型对齐。

- Collator（按阶段）：`src/llamafactory/data/collator.py`
  - `pt`: `DataCollatorForLanguageModeling(mlm=False)`
  - `sft`: `SFTDataCollatorWith4DAttentionMask`（支持4D注意力、block-diag、label pad 等）
  - `ppo`: `MultiModalDataCollatorForSeq2Seq`（生成时/训练时左右 padding 策略切换）
  - `rm`: `PairwiseDataCollatorWithPadding`
  - `kto`: `KTODataCollatorWithPadding`

### 模型加载与适配

- 统一入口：`src/llamafactory/model/loader.py: load_model(...)`
  1. 加载配置：`load_config(model_args)` → `patch_config(...)`（注入 rope/模板、训练开关等）→ `apply_liger_kernel(...)` 可选。
  2. 选择模型类型并加载：根据配置类型自动选 `AutoModelForCausalLM` / `AutoModelForVision2Seq` / `AutoModelForImageTextToText` / `AutoModelForSeq2SeqLM` / `AutoModelForTextToWaveform`，支持从 config 初始化或从预训练权重加载，包含特例（如 `qwen2_5_omni` 使用 `thinker` 部分）。
  3. `patch_model(...)` 与 `register_autoclass(...)`：注册必要的钩子、特殊行为与保存/加载兼容。
  4. 适配器初始化：`init_adapter(...)` in `src/llamafactory/model/adapter.py`
     - 支持 `full`、`freeze`、`lora`/`oft`。
     - LoRA 路径：
       - 合并已有 adapter（可多路）→ `merge_and_unload()`（量化/ZeRO3/Unsloth 场景约束不同）。
       - 继续训练时加载需恢复的 adapter；新训练时基于 `lora_target`/`find_all_linear_modules` 建立新 Adapter。
       - 支持 DoRA、Llama-Pro 扩展模块选择、PISSA 初始化等约束与校验。
     - QLoRA/量化：根据 `ModelArguments.quantization_bit` 与 `FinetuningArguments` 自动处理可训练参数精度（`pure_bf16`、ZeRO3 等）。
  5. 值头：若 `add_valuehead=True`（PPO/RM 等阶段需要），则附加 `AutoModelForCausalLMWithValueHead`。

### 训练编排（各阶段工作流）

通用骨架：`load_tokenizer` → `get_template_and_fix_tokenizer` → `get_dataset` → `load_model` → `data_collator` → 自定义 Trainer → 训练/评估/保存。

- `pt`（预训练）: `src/llamafactory/train/pt/workflow.py`
  - Collator: `DataCollatorForLanguageModeling(mlm=False)`
  - Trainer: `CustomTrainer`（继承 HF `Trainer`，定制优化器/调度器、顺序采样等）
  - 训练后保存模型、状态，按需绘制 loss 曲线。

- `sft`（监督微调）: `src/llamafactory/train/sft/workflow.py`
  - Collator: `SFTDataCollatorWith4DAttentionMask`
  - Metrics: 
    - 若 `predict_with_generate=True`：`ComputeSimilarity`
    - 否则可启用 `compute_accuracy`：`ComputeAccuracy` + `eval_logit_processor`
  - 生成参数：由 `GeneratingArguments.to_dict` 得到 `gen_kwargs`，补全 `eos_token_id` 与 `pad_token_id`。
  - Trainer: `CustomSeq2SeqTrainer`
  - 支持 `do_train`、`do_eval`、`do_predict`，可选统计有效 tokens/s，绘图等。

- `rm`（奖励模型）: `src/llamafactory/train/rm/workflow.py`
  - Collator: `PairwiseDataCollatorWithPadding`
  - Trainer: `PairwiseTrainer`（重写 `compute_loss` 计算 pairwise loss，自动 value head 修复回调）。

- `ppo`（强化学习）: `src/llamafactory/train/ppo/workflow.py`
  - 额外创建 `ref_model` 与 `reward_model`：`create_ref_model(...)`、`create_reward_model(...)`。
  - Collator: `MultiModalDataCollatorForSeq2Seq`（生成时左 padding）。
  - Trainer: `CustomPPOTrainer`（自定义 `ppo_train` 循环，按 mini-batch 生成、打分、PPO 更新）。

- `dpo`（直接偏好优化）: `src/llamafactory/train/dpo/workflow.py`
  - Trainer: `CustomDPOTrainer`（基于 TRL DPOTrainer 扩展）。

- `kto`（KTO 反馈优化）: `src/llamafactory/train/kto/workflow.py`
  - 参考模型：预测-only 时可直接复用当前模型为 `ref_model`，否则通过 `create_ref_model` 创建。
  - Collator: `KTODataCollatorWithPadding`。
  - Trainer: `CustomKTOTrainer`。

### 典型 SFT 流程（简）

1. 解析参数：`get_train_args`（含模板、packing、train_on_prompt、mask_history 等校验）。
2. `load_tokenizer` → `get_template_and_fix_tokenizer`。
3. `get_dataset`：合并/预处理/划分，必要时保存分词结果。
4. `load_model`：加载基础模型→打补丁→初始化 LoRA/冻结/全参→注册类。
5. 构造 `SFTDataCollatorWith4DAttentionMask`，准备指标与 `gen_kwargs`。
6. `CustomSeq2SeqTrainer`：训练→保存→评估/预测→指标与曲线。

### 关键特性与注意事项

- **分布式**: 训练需分布式启动（`torchrun` 或 `llamafactory-cli`），否则参数校验会报错。
- **预测与打分**: 非 SFT 阶段禁止 `predict_with_generate`；SFT 的 `do_predict` 必须配合 `predict_with_generate=True`。
- **Packing**: SFT 可启用 `packing/neat_packing`；`neat_packing` 会对 `datasets` 的内部类型作 hack 以获得 int32 attention mask。
- **LoRA 合并/恢复**: 量化/ZeRO3/Unsloth 场景对多适配器合并或恢复有约束。
- **QLoRA 精度**: 在非 `pure_bf16` 且非 ZeRO3 情况下，会将可训练参数提升到 fp32（以避免数值不稳）。
- **PPO 限制**: 仅支持训练，不支持评估；不兼容 S^2-Attn；日志仅支持 `wandb` 或 `tensorboard`。

### 最小化运行示例（SFT）

假设已准备好 YAML 配置（见 `examples/train_lora/llama3_lora_sft.yaml`），使用分布式启动：

```bash
torchrun --nproc_per_node=8 -m src.train examples/train_lora/llama3_lora_sft.yaml
```

或以 CLI 形式（等价传参）传入关键参数：

```bash
torchrun --nproc_per_node=8 -m src.train \
  stage sft \
  do_train true \
  model_name_or_path meta-llama/Meta-Llama-3-8B \
  dataset_dir ./data \
  dataset alpaca_en_demo \
  finetuning_type lora \
  template llama3 \
  per_device_train_batch_size 1 \
  gradient_accumulation_steps 16 \
  learning_rate 2e-4 \
  num_train_epochs 3 \
  output_dir ./output/llama3-lora-sft
```

### 文件与模块速查

- 训练调度：`src/llamafactory/train/tuner.py`
- 参数解析与校验：`src/llamafactory/hparams/parser.py`
- 数据加载/预处理：`src/llamafactory/data/loader.py`
- 数据处理器：`src/llamafactory/data/processor/*`
- Collator：`src/llamafactory/data/collator.py`
- 模型加载与适配器：`src/llamafactory/model/loader.py`、`src/llamafactory/model/adapter.py`
- 各阶段工作流：`src/llamafactory/train/*/workflow.py`
- 自定义 Trainer：`src/llamafactory/train/*/trainer.py`

### 参数说明（常用）

- **ModelArguments**（模型相关）
  - `model_name_or_path`: 基座模型权重路径或 Hugging Face 仓库名。
  - `cache_dir`: 模型缓存目录。
  - `trust_remote_code`: 允许自定义模型代码。
  - `quantization_bit`: 量化位宽（如 4/8，用于 QLoRA/PTQ）。
  - `rope_scaling`: RoPE 扩展配置。
  - `use_unsloth` / `enable_liger_kernel` / `flash_attn`: 性能加速相关开关。
  - `adapter_name_or_path`/`adapter_folder`: 已有 LoRA/OFT 适配器路径（可多路合并或恢复）。

- **DataArguments**（数据与预处理）
  - `dataset_dir`: 数据集根目录（JSON/JSONL/TXT 等，详见 `data/README.md`）。
  - `dataset` / `eval_dataset`: 训练/评估数据集名称（可多项，逗号分隔）。
  - `cutoff_len`: 最大序列长度。
  - `packing` / `neat_packing`: 打包策略（SFT 阶段可用）。
  - `train_on_prompt` / `mask_history`: 训练是否包含提示、是否遮蔽历史（仅 SFT 合法）。
  - `preprocessing_num_workers` / `preprocessing_batch_size`: `datasets.map` 并行与批大小。
  - `tokenized_path`: 若提供则优先加载已分词数据；也可在首次分词后保存。
  - `eval_on_each_dataset`: 多评估集时分别汇报指标。

- **TrainingArguments**（训练通用）
  - `do_train`/`do_eval`/`do_predict`: 开启对应阶段。
  - `per_device_train_batch_size` / `per_device_eval_batch_size`: 设备级批大小。
  - `gradient_accumulation_steps`: 梯度累计步数。
  - `learning_rate` / `lr_scheduler_type` / `warmup_steps` / `max_grad_norm`。
  - `num_train_epochs` / `max_steps`: 训练轮数或总步数（二选一优先）。
  - `logging_steps` / `save_steps`: 日志与保存频率。
  - `output_dir`: 输出目录。
  - `report_to`: 监控平台（`wandb`、`tensorboard`）。
  - `predict_with_generate`: SFT 预测/评估启用生成。
  - 分布式：建议使用 `torchrun`/`llamafactory-cli` 启动（单机多卡/多机多卡）。

- **FinetuningArguments**（微调策略）
  - `stage`: `pt|sft|rm|ppo|dpo|kto`。
  - `finetuning_type`: `full|freeze|lora`（含 `oft`）。
  - `lora_target` / `use_dora` / `pissa_init` / `use_llama_pro`: LoRA/DoRA/PISSA/LLama-Pro 相关。
  - `compute_accuracy`: SFT 评估阶段是否计算 token-level accuracy。
  - `disable_shuffling`: 训练集是否禁用打乱（可复现/对齐需求）。
  - PPO/RLHF 相关：`reward_model_type`、`early_stopping_steps` 等。

- **GeneratingArguments**（生成与评测）
  - `max_new_tokens` / `temperature` / `top_p` / `top_k` / `num_beams`。
  - `repetition_penalty` / `length_penalty` / `do_sample`。
  - SFT 中会转为 `gen_kwargs` 传入 `Seq2SeqTrainer.evaluate/predict`。

> 提示：完整字段与默认值见 `src/llamafactory/hparams/*`，WebUI 参数映射见 `src/llamafactory/webui/runner.py`。

### 评估流程与指标

- **内置评估（训练期间）**
  - SFT：
    - 若 `predict_with_generate=True`：使用 `ComputeSimilarity`（语义相似度类指标）。
    - 若 `compute_accuracy=True`：计算 token-level accuracy，配合 `eval_logit_processor`。
    - 评估入口：`CustomSeq2SeqTrainer.evaluate(metric_key_prefix="eval", **gen_kwargs)`。
  - PT：默认 `Trainer.evaluate()` 汇报 loss。
  - RM：`PairwiseTrainer` 返回 `eval_loss` 等（成对偏好损失）。
  - PPO/DPO/KTO：训练目标不同，通常不在同一 Trainer 上做生成式评估；PPO 阶段不支持 `load_best_model_at_end` 与 eval。

- **外部任务评测脚本（离线评测）**
  - 学术基准目录：`evaluation/`
    - `evaluation/mmlu/mmlu.py`
    - `evaluation/cmmlu/cmmlu.py`
    - `evaluation/ceval/ceval.py`
  - 用法示例（根据实际脚本参数补全）：

```bash
# 导出/合并权重（若是 LoRA，需要先合并或在推理时挂载适配器）
# 之后以模型路径运行基准评测
python evaluation/mmlu/mmlu.py \
  --model_name_or_path ./output/llama3-lora-sft \
  --tokenizer_name ./output/llama3-lora-sft \
  --batch_size 8 \
  --max_new_tokens 128

python evaluation/cmmlu/cmmlu.py \
  --model_name_or_path ./output/llama3-lora-sft \
  --batch_size 8

python evaluation/ceval/ceval.py \
  --model_name_or_path ./output/llama3-lora-sft \
  --batch_size 8
```

- **推理与快速验证**
  - 可使用 `src/api.py`/`src/webui.py` 或 `scripts/vllm_infer.py` 做快速推理验证；亦可参照 `examples/inference/*.yaml` 配置生成效果。

- **可视化与日志**
  - 若启用 `report_to=wandb|tensorboard`，在训练与评估过程中会自动汇报 `loss/accuracy/学习率` 等；
  - SFT/PT 结束后可选择绘制 loss 曲线（`plot_loss`）。
