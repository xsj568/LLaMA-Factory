#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLaMA-Factory FastAPI æœåŠ¡éƒ¨ç½²
==============================

æä¾›åŸºäº FastAPI çš„æ¨¡å‹æ¨ç†æœåŠ¡ï¼Œæ”¯æŒï¼š
- æ–‡æœ¬ç”Ÿæˆ
- å¯¹è¯èŠå¤©
- æ‰¹é‡å¤„ç†
- å¥åº·æ£€æŸ¥
- å¯åŠ¨å‰ç¯å¢ƒæ£€æŸ¥

ä½¿ç”¨æ–¹æ³•ï¼š
python fastapi_service.py
"""

import os
import sys
import json
import asyncio
import subprocess
from pathlib import Path
from typing import List, Optional, Dict, Any
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
THIS_FILE = Path(__file__).resolve()
LOCAL_RUN_DIR = THIS_FILE.parent  # å½“å‰ç›®å½•å°±æ˜¯ local_run
PROJECT_ROOT = LOCAL_RUN_DIR.parent  # ä¸Šçº§ç›®å½•æ˜¯ LLaMA-Factory
SRC_DIR = PROJECT_ROOT / "src"

if str(SRC_DIR) not in sys.path:
    sys.path.insert(0, str(SRC_DIR))

# FastAPI ç›¸å…³å¯¼å…¥
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
import uvicorn

# LLaMA-Factory å¯¼å…¥
from llamafactory.chat.chat_model import ChatModel

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(str(LOCAL_RUN_DIR / 'fastapi_service.log'), encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡
chat_model = None
model_loaded = False

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…"""
    required_packages = [
        "fastapi",
        "uvicorn",
        "pydantic"
    ]
    
    missing_packages = []
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)
    
    if missing_packages:
        print(f"âŒ ç¼ºå°‘ä»¥ä¸‹ä¾èµ–åŒ…: {', '.join(missing_packages)}")
        print("è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:")
        print(f"pip install {' '.join(missing_packages)}")
        return False
    
    return True

def check_model_files():
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    model_path = LOCAL_RUN_DIR / "models" / "Llama-3.2-1B-Instruct"
    adapter_path = LOCAL_RUN_DIR / "saves" / "llama3.2-1b-lora-sft"
    
    if not model_path.exists():
        print(f"âŒ é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("è¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å·²æ­£ç¡®æ”¾ç½®åœ¨ models/ ç›®å½•ä¸‹")
        return False
    
    if not adapter_path.exists():
        print(f"âš ï¸  è­¦å‘Š: LoRA é€‚é…å™¨ä¸å­˜åœ¨: {adapter_path}")
        print("å°†ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œæ¨ç†")
    else:
        print(f"âœ… LoRA é€‚é…å™¨å­˜åœ¨: {adapter_path}")
    
    return True

def show_startup_info():
    """æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯"""
    print("=" * 60)
    print("ğŸš€ LLaMA-Factory FastAPI æœåŠ¡")
    print("=" * 60)
    print("ğŸ“‹ æœåŠ¡ä¿¡æ¯:")
    print(f"   â€¢ æœåŠ¡åœ°å€: http://localhost:8000")
    print(f"   â€¢ API æ–‡æ¡£: http://localhost:8000/docs")
    print(f"   â€¢ å¥åº·æ£€æŸ¥: http://localhost:8000/health")
    print(f"   â€¢ æ¨¡å‹ä¿¡æ¯: http://localhost:8000/model/info")
    print()
    print("ğŸ”§ å¯ç”¨æ¥å£:")
    print("   â€¢ POST /chat      - èŠå¤©å¯¹è¯")
    print("   â€¢ POST /generate  - æ–‡æœ¬ç”Ÿæˆ")
    print("   â€¢ POST /batch     - æ‰¹é‡å¤„ç†")
    print()
    print("ğŸ“ æµ‹è¯•å‘½ä»¤:")
    print("   â€¢ å¥åº·æ£€æŸ¥: curl http://localhost:8000/health")
    print("   â€¢ ç®€å•èŠå¤©: curl -X POST http://localhost:8000/chat \\")
    print("     -H 'Content-Type: application/json' \\")
    print("     -d '{\"messages\":[{\"role\":\"user\",\"content\":\"ä½ å¥½\"}]}'")
    print()
    print("â¹ï¸  æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    print("=" * 60)

# Pydantic æ¨¡å‹å®šä¹‰
class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚æ¨¡å‹"""
    messages: List[Dict[str, str]] = Field(..., description="å¯¹è¯æ¶ˆæ¯åˆ—è¡¨")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="ç”Ÿæˆæ¸©åº¦")
    max_tokens: int = Field(default=512, ge=1, le=4096, description="æœ€å¤§ç”Ÿæˆtokenæ•°")
    top_p: float = Field(default=0.9, ge=0.0, le=1.0, description="æ ¸é‡‡æ ·å‚æ•°")
    stream: bool = Field(default=False, description="æ˜¯å¦æµå¼è¾“å‡º")

class TextGenerationRequest(BaseModel):
    """æ–‡æœ¬ç”Ÿæˆè¯·æ±‚æ¨¡å‹"""
    prompt: str = Field(..., description="è¾“å…¥æç¤ºè¯")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="ç”Ÿæˆæ¸©åº¦")
    max_tokens: int = Field(default=512, ge=1, le=4096, description="æœ€å¤§ç”Ÿæˆtokenæ•°")
    top_p: float = Field(default=0.9, ge=0.0, le=1.0, description="æ ¸é‡‡æ ·å‚æ•°")
    stream: bool = Field(default=False, description="æ˜¯å¦æµå¼è¾“å‡º")

class BatchRequest(BaseModel):
    """æ‰¹é‡å¤„ç†è¯·æ±‚æ¨¡å‹"""
    prompts: List[str] = Field(..., description="æç¤ºè¯åˆ—è¡¨")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="ç”Ÿæˆæ¸©åº¦")
    max_tokens: int = Field(default=512, ge=1, le=4096, description="æœ€å¤§ç”Ÿæˆtokenæ•°")
    top_p: float = Field(default=0.9, ge=0.0, le=1.0, description="æ ¸é‡‡æ ·å‚æ•°")

class ChatResponse(BaseModel):
    """èŠå¤©å“åº”æ¨¡å‹"""
    response: str = Field(..., description="æ¨¡å‹å›å¤")
    usage: Dict[str, int] = Field(..., description="tokenä½¿ç”¨ç»Ÿè®¡")
    model: str = Field(..., description="æ¨¡å‹åç§°")

class TextGenerationResponse(BaseModel):
    """æ–‡æœ¬ç”Ÿæˆå“åº”æ¨¡å‹"""
    generated_text: str = Field(..., description="ç”Ÿæˆçš„æ–‡æœ¬")
    usage: Dict[str, int] = Field(..., description="tokenä½¿ç”¨ç»Ÿè®¡")
    model: str = Field(..., description="æ¨¡å‹åç§°")

class BatchResponse(BaseModel):
    """æ‰¹é‡å¤„ç†å“åº”æ¨¡å‹"""
    results: List[str] = Field(..., description="ç”Ÿæˆç»“æœåˆ—è¡¨")
    usage: Dict[str, int] = Field(..., description="tokenä½¿ç”¨ç»Ÿè®¡")
    model: str = Field(..., description="æ¨¡å‹åç§°")

class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”æ¨¡å‹"""
    status: str = Field(..., description="æœåŠ¡çŠ¶æ€")
    model_loaded: bool = Field(..., description="æ¨¡å‹æ˜¯å¦å·²åŠ è½½")
    model_name: Optional[str] = Field(None, description="æ¨¡å‹åç§°")

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="LLaMA-Factory API æœåŠ¡",
    description="åŸºäº LLaMA-Factory çš„æ¨¡å‹æ¨ç† API æœåŠ¡",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# æ·»åŠ  CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒè¯·è®¾ç½®å…·ä½“çš„åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def load_model():
    """åŠ è½½æ¨¡å‹"""
    global chat_model, model_loaded
    
    try:
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        model_path = LOCAL_RUN_DIR / "models" / "Llama-3.2-1B-Instruct"
        adapter_path = LOCAL_RUN_DIR / "saves" / "llama3.2-1b-lora-sft"
        
        if not model_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        
        # æ„å»ºæ¨¡å‹å‚æ•°
        model_args = {
            "model_name_or_path": str(model_path),
            "template": "llama3",
            "trust_remote_code": True,
        }
        
        # å¦‚æœå­˜åœ¨é€‚é…å™¨ï¼Œåˆ™åŠ è½½
        if adapter_path.exists():
            model_args["adapter_name_or_path"] = str(adapter_path)
            logger.info(f"åŠ è½½ LoRA é€‚é…å™¨: {adapter_path}")
        
        # åˆ›å»ºèŠå¤©æ¨¡å‹
        chat_model = ChatModel(args=model_args)
        model_loaded = True
        
        logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
        if adapter_path.exists():
            logger.info(f"LoRA é€‚é…å™¨åŠ è½½æˆåŠŸ: {adapter_path}")
            
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        model_loaded = False
        raise e

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶"""
    logger.info("æ­£åœ¨å¯åŠ¨ FastAPI æœåŠ¡...")
    try:
        load_model()
        logger.info("FastAPI æœåŠ¡å¯åŠ¨æˆåŠŸï¼")
    except Exception as e:
        logger.error(f"æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        raise e

@app.get("/", response_model=Dict[str, str])
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "LLaMA-Factory API æœåŠ¡",
        "version": "1.0.0",
        "docs": "/docs",
        "health": "/health"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return HealthResponse(
        status="healthy" if model_loaded else "unhealthy",
        model_loaded=model_loaded,
        model_name="Llama-3.2-1B-Instruct" if model_loaded else None
    )

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """èŠå¤©æ¥å£"""
    if not model_loaded:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    try:
        # è®¾ç½®ç”Ÿæˆå‚æ•°
        generation_kwargs = {
            "temperature": request.temperature,
            "max_new_tokens": request.max_tokens,
            "top_p": request.top_p,
        }
        
        # å¤„ç†æ¶ˆæ¯ï¼Œåˆ†ç¦»ç³»ç»Ÿæç¤ºè¯å’Œå¯¹è¯æ¶ˆæ¯
        system_message = None
        chat_messages = []
        
        for msg in request.messages:
            if msg["role"] == "system":
                system_message = msg["content"]
            else:
                chat_messages.append(msg)
        
        # ç”Ÿæˆå›å¤
        if request.stream:
            # æµå¼ç”Ÿæˆ
            response_text = ""
            for new_text in chat_model.stream_chat(chat_messages, system=system_message, **generation_kwargs):
                response_text += new_text
            # æµå¼ç”Ÿæˆæ—¶ä½¿ç”¨ç®€åŒ–è®¡ç®—
            input_tokens = sum(len(str(msg["content"]).split()) for msg in chat_messages)
            if system_message:
                input_tokens += len(str(system_message).split())
            output_tokens = len(response_text.split())
        else:
            # éæµå¼ç”Ÿæˆ
            response_list = chat_model.chat(chat_messages, system=system_message, **generation_kwargs)
            response_obj = response_list[0]
            response_text = response_obj.response_text
            # ä½¿ç”¨ Response å¯¹è±¡ä¸­çš„å‡†ç¡® token è®¡æ•°
            input_tokens = response_obj.prompt_length
            output_tokens = response_obj.response_length
        
        return ChatResponse(
            response=response_text,
            usage={
                "prompt_tokens": input_tokens,
                "completion_tokens": output_tokens,
                "total_tokens": input_tokens + output_tokens
            },
            model="Llama-3.2-1B-Instruct"
        )
        
    except Exception as e:
        logger.error(f"èŠå¤©ç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå¤±è´¥: {str(e)}")

@app.post("/generate", response_model=TextGenerationResponse)
async def generate_text(request: TextGenerationRequest):
    """æ–‡æœ¬ç”Ÿæˆæ¥å£"""
    if not model_loaded:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    try:
        # å°†æç¤ºè¯è½¬æ¢ä¸ºæ¶ˆæ¯æ ¼å¼
        messages = [{"role": "user", "content": request.prompt}]
        
        # è®¾ç½®ç”Ÿæˆå‚æ•°
        generation_kwargs = {
            "temperature": request.temperature,
            "max_new_tokens": request.max_tokens,
            "top_p": request.top_p,
        }
        
        # ç”Ÿæˆæ–‡æœ¬
        if request.stream:
            # æµå¼ç”Ÿæˆ
            generated_text = ""
            for new_text in chat_model.stream_chat(messages, **generation_kwargs):
                generated_text += new_text
            # æµå¼ç”Ÿæˆæ—¶ä½¿ç”¨ç®€åŒ–è®¡ç®—
            input_tokens = len(request.prompt.split())
            output_tokens = len(generated_text.split())
        else:
            # éæµå¼ç”Ÿæˆ
            response_list = chat_model.chat(messages, **generation_kwargs)
            response_obj = response_list[0]
            generated_text = response_obj.response_text
            # ä½¿ç”¨ Response å¯¹è±¡ä¸­çš„å‡†ç¡® token è®¡æ•°
            input_tokens = response_obj.prompt_length
            output_tokens = response_obj.response_length
        
        return TextGenerationResponse(
            generated_text=generated_text,
            usage={
                "prompt_tokens": input_tokens,
                "completion_tokens": output_tokens,
                "total_tokens": input_tokens + output_tokens
            },
            model="Llama-3.2-1B-Instruct"
        )
        
    except Exception as e:
        logger.error(f"æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå¤±è´¥: {str(e)}")

@app.post("/batch", response_model=BatchResponse)
async def batch_generate(request: BatchRequest):
    """æ‰¹é‡ç”Ÿæˆæ¥å£"""
    if not model_loaded:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    try:
        results = []
        total_input_tokens = 0
        total_output_tokens = 0
        
        # è®¾ç½®ç”Ÿæˆå‚æ•°
        generation_kwargs = {
            "temperature": request.temperature,
            "max_new_tokens": request.max_tokens,
            "top_p": request.top_p,
        }
        
        # æ‰¹é‡å¤„ç†
        for prompt in request.prompts:
            messages = [{"role": "user", "content": prompt}]
            response_list = chat_model.chat(messages, **generation_kwargs)
            response_obj = response_list[0]
            generated_text = response_obj.response_text
            results.append(generated_text)
            
            # ç´¯è®¡ token ä½¿ç”¨é‡ï¼ˆä½¿ç”¨ Response å¯¹è±¡ä¸­çš„å‡†ç¡®è®¡æ•°ï¼‰
            total_input_tokens += response_obj.prompt_length
            total_output_tokens += response_obj.response_length
        
        return BatchResponse(
            results=results,
            usage={
                "prompt_tokens": total_input_tokens,
                "completion_tokens": total_output_tokens,
                "total_tokens": total_input_tokens + total_output_tokens
            },
            model="Llama-3.2-1B-Instruct"
        )
        
    except Exception as e:
        logger.error(f"æ‰¹é‡ç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡ç”Ÿæˆå¤±è´¥: {str(e)}")

@app.get("/model/info")
async def model_info():
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    if not model_loaded:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    return {
        "model_name": "Llama-3.2-1B-Instruct",
        "model_path": str(LOCAL_RUN_DIR / "models" / "Llama-3.2-1B-Instruct"),
        "adapter_path": str(LOCAL_RUN_DIR / "saves" / "llama3.2-1b-lora-sft"),
        "template": "llama3",
        "status": "loaded"
    }

def main():
    """ä¸»å‡½æ•°ï¼šå¯åŠ¨å‰æ£€æŸ¥å¹¶å¯åŠ¨æœåŠ¡"""
    print("ğŸ” æ­£åœ¨æ£€æŸ¥ç¯å¢ƒ...")
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        print("\nâŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·å®‰è£…ç¼ºå¤±çš„ä¾èµ–åé‡è¯•")
        sys.exit(1)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not check_model_files():
        print("\nâŒ æ¨¡å‹æ–‡ä»¶æ£€æŸ¥å¤±è´¥ï¼Œè¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å­˜åœ¨")
        sys.exit(1)
    
    print("âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡")
    print()
    
    # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
    show_startup_info()
    
    try:
        # å¯åŠ¨æœåŠ¡
        print("ğŸš€ æ­£åœ¨å¯åŠ¨æœåŠ¡...")
        uvicorn.run(
            "fastapi_service:app",
            host="0.0.0.0",
            port=8000,
            reload=False,
            log_level="info"
        )
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  æœåŠ¡å·²åœæ­¢")
    except Exception as e:
        print(f"\nâŒ æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
