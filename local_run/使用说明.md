# Gemma 模型部署服务

## 文件说明

- `gemma_service.py` - 主服务文件（包含所有功能）
- `config.yaml` - 配置文件（定义不同环境的模型和参数）
- `start_local.py` - 启动本地环境脚本
- `start_server.py` - 启动生产环境脚本

## 使用方法（基于配置文件启动）

### 1. 安装依赖
```bash
pip install fastapi uvicorn pydantic pyyaml transformers torch
```

### 2. 选择配置文件
- `config.local.yaml`：本地开发（HuggingFace 推理，Gemma 3 4B）
- `config.production.yaml`：生产部署（vLLM 推理，Gemma 2 27B）

### 3. 启动服务

#### 本地环境（使用 config.local.yaml）
```bash
python start_local.py
```

#### 生产环境（使用 config.production.yaml）
```bash
python start_server.py
```

#### 自定义配置文件路径
```bash
# 设置配置文件路径启动（Windows PowerShell）
$env:GEMMA_CONFIG = "D:/path/to/custom.yaml"
python gemma_service.py
```

### 4. 测试服务

访问 http://localhost:8000/docs 查看 API 文档

测试聊天：
```bash
curl -X POST http://localhost:8000/chat \
  -H 'Content-Type: application/json' \
  -d '{"messages":[{"role":"user","content":"你好"}]}'
```

## 配置说明

- **环境配置**: 通过 `config.yaml` 完全控制，支持任意环境名称
- **模型路径**: 支持本地路径和 HuggingFace 模型名称
- **推理引擎**: 支持 HuggingFace 和 vLLM
- **日志系统**: 所有输出都使用日志记录，便于调试和监控
