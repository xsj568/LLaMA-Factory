# 推理服务统一配置文件
# =====================

# 当前使用的模型 - 直接选择模型名字
current_model: "qwen3_0_6b"  # 可选: qwen3_0_6b, llama3_2_1b

# 支持的模型列表
supported_models:
  qwen3_0_6b:
    name: "Qwen3-0.6B"
    template: "qwen3"
    base_path: "models/Qwen3-0.6B"
    merged_path: "output/Qwen3-0.6B-loral-sft"
    description: "Qwen3-0.6B 模型，支持思考模式"
    
  llama3_2_1b:
    name: "Llama-3.2-1B-Instruct"
    template: "llama3"
    base_path: "models/Llama-3.2-1B-Instruct"
    merged_path: "output/llama3.2-1b-lora-sft"
    description: "Llama-3.2-1B-Instruct 模型，标准对话模式"

# 推理引擎配置
inference_engines:
  default: "vllm"  # 默认使用vllm
  fallback: "huggingface"  # vllm不可用时回退到huggingface
  
  vllm_config:
    maxlen: 4096
    gpu_util: 0.7
    enforce_eager: false
    max_lora_rank: 32

# 生成参数默认值
generation_defaults:
  temperature: 0.7
  max_tokens: 512
  top_p: 0.9
  stream: false

# 服务配置
service_config:
  host: "0.0.0.0"
  port: 8000
  default_use_merged: false
  default_use_vllm: false
