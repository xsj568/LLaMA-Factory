# LLaMA-Factory é…ç½®æ–‡ä»¶è¯´æ˜

## ğŸ“ é…ç½®æ–‡ä»¶ç»“æ„

```
configs/
â”œâ”€â”€ README.md                    # æœ¬è¯´æ˜æ–‡ä»¶
â”œâ”€â”€ runner_config.yaml          # ç»Ÿä¸€è¿è¡Œé…ç½®æ–‡ä»¶
â”œâ”€â”€ llama3.2_lora_sft_local.yaml # Llama-3.2-1B LoRA å¾®è°ƒé…ç½®
â”œâ”€â”€ qwen2.5_lora_sft_local.yaml  # Qwen-2.5 LoRA å¾®è°ƒé…ç½®
â””â”€â”€ LLaMA_Factory_API.postman_collection.json # API æµ‹è¯•é›†åˆ
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. è®­ç»ƒæ¨¡å‹
```bash
# ä½¿ç”¨é»˜è®¤é…ç½® (Llama-3.2-1B LoRA å¾®è°ƒ)
python llamafactory_local_runner.py

# æˆ–æŒ‡å®šé…ç½®æ–‡ä»¶
python llamafactory_local_runner.py train --config configs/llama3.2_lora_sft_local.yaml
```

### 2. èŠå¤©æµ‹è¯•
```bash
# ä¿®æ”¹ runner_config.yaml ä¸­çš„ command ä¸º chat
python llamafactory_local_runner.py
```

### 3. Web UI
```bash
# ä¿®æ”¹ runner_config.yaml ä¸­çš„ command ä¸º webui
python llamafactory_local_runner.py
```

## âš™ï¸ é…ç½®æ–‡ä»¶è¯¦è§£

### runner_config.yaml
ç»Ÿä¸€è¿è¡Œé…ç½®æ–‡ä»¶ï¼Œæ§åˆ¶æ‰€æœ‰è¿è¡Œåœºæ™¯ï¼š

```yaml
# å½“å‰æ¿€æ´»é…ç½®
command: train  # å‘½ä»¤ç±»å‹
config: llama3.2_lora_sft_local.yaml  # é…ç½®æ–‡ä»¶
args:  # å…¨å±€å‚æ•°
  cache_dir: hf_cache
  dataset_dir: ../data
```

**æ”¯æŒçš„å‘½ä»¤ç±»å‹ï¼š**
- `train`: è®­ç»ƒæ¨¡å‹
- `chat`: CLI èŠå¤©
- `webchat`: Web èŠå¤©
- `api`: API æœåŠ¡
- `webui`: Web UI
- `export`: æ¨¡å‹å¯¼å‡º
- `eval`: æ¨¡å‹è¯„ä¼°

### llama3.2_lora_sft_local.yaml
Llama-3.2-1B-Instruct æ¨¡å‹çš„ LoRA å¾®è°ƒé…ç½®ï¼š

**ä¸»è¦é…ç½®é¡¹ï¼š**
- **æ¨¡å‹é…ç½®**: æ¨¡å‹è·¯å¾„ã€ä¿¡ä»»è¿œç¨‹ä»£ç 
- **å¾®è°ƒæ–¹æ³•**: LoRA å‚æ•° (rank=8, alpha=16, dropout=0.1)
- **æ•°æ®é›†é…ç½®**: æ•°æ®é›†ã€æ¨¡æ¿ã€æˆªæ–­é•¿åº¦
- **è®­ç»ƒå‚æ•°**: æ‰¹æ¬¡å¤§å°ã€å­¦ä¹ ç‡ã€è®­ç»ƒè½®æ•°
- **è¾“å‡ºé…ç½®**: ä¿å­˜è·¯å¾„ã€æ—¥å¿—é—´éš”

**å…³é”®å‚æ•°è¯´æ˜ï¼š**
```yaml
lora_rank: 8          # LoRA ç§©ï¼Œæ§åˆ¶å‚æ•°é‡ (æ¨è: 8-16)
lora_alpha: 16        # LoRA ç¼©æ”¾å‚æ•°ï¼Œé€šå¸¸ä¸º rank çš„ 2 å€
lora_target: q_proj,v_proj  # LoRA ä½œç”¨æ¨¡å—
learning_rate: 1.0e-4 # å­¦ä¹ ç‡ (LoRA æ¨è: 1e-4 åˆ° 5e-4)
max_samples: 1000     # æœ€å¤§æ ·æœ¬æ•° (æµ‹è¯•æ—¶å¯è®¾ä¸º 10)
```

## ğŸ”§ é…ç½®ä¿®æ”¹æŒ‡å—

### 1. åˆ‡æ¢æ¨¡å‹
ä¿®æ”¹ `runner_config.yaml` ä¸­çš„ `config` å­—æ®µï¼š
```yaml
config: llama3.2_lora_sft_local.yaml  # Llama æ¨¡å‹
# config: qwen2.5_lora_sft_local.yaml  # Qwen æ¨¡å‹
```

### 2. è°ƒæ•´è®­ç»ƒå‚æ•°
ä¿®æ”¹å¯¹åº”çš„ YAML é…ç½®æ–‡ä»¶ï¼š
```yaml
# å¿«é€Ÿæµ‹è¯•
max_samples: 10
num_train_epochs: 1.0
save_steps: 2

# æ­£å¼è®­ç»ƒ
max_samples: 1000
num_train_epochs: 3.0
save_steps: 500
```

### 3. è°ƒæ•´ LoRA å‚æ•°
```yaml
# è½»é‡çº§é…ç½® (å‚æ•°é‡å°‘)
lora_rank: 4
lora_alpha: 8
lora_target: q_proj,v_proj

# æ ‡å‡†é…ç½® (æ¨è)
lora_rank: 8
lora_alpha: 16
lora_target: q_proj,v_proj

# å®Œæ•´é…ç½® (å‚æ•°é‡å¤š)
lora_rank: 16
lora_alpha: 32
lora_target: q_proj,k_proj,v_proj,o_proj
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. å†…å­˜ä¼˜åŒ–
```yaml
per_device_train_batch_size: 1  # å‡å°æ‰¹æ¬¡å¤§å°
gradient_accumulation_steps: 8  # å¢åŠ æ¢¯åº¦ç´¯ç§¯
bf16: true  # ä½¿ç”¨ bfloat16 ç²¾åº¦
```

### 2. è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–
```yaml
dataloader_num_workers: 4  # å¢åŠ æ•°æ®åŠ è½½è¿›ç¨‹ (Linux/Mac)
preprocessing_num_workers: 4  # å¢åŠ é¢„å¤„ç†è¿›ç¨‹
```

### 3. æ¨¡å‹è´¨é‡ä¼˜åŒ–
```yaml
learning_rate: 5.0e-4  # é€‚å½“æé«˜å­¦ä¹ ç‡
num_train_epochs: 5.0  # å¢åŠ è®­ç»ƒè½®æ•°
warmup_ratio: 0.1  # å­¦ä¹ ç‡é¢„çƒ­
```

## ğŸ› å¸¸è§é—®é¢˜

### 1. å†…å­˜ä¸è¶³
- å‡å° `per_device_train_batch_size`
- å¢åŠ  `gradient_accumulation_steps`
- å¯ç”¨ `bf16: true`

### 2. è®­ç»ƒé€Ÿåº¦æ…¢
- å¢åŠ  `dataloader_num_workers`
- å‡å°‘ `max_samples` è¿›è¡Œæµ‹è¯•
- ä½¿ç”¨æ›´å°çš„ `lora_rank`

### 3. æ¨¡å‹è´¨é‡å·®
- å¢åŠ  `num_train_epochs`
- è°ƒæ•´ `learning_rate`
- ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®

## ğŸ“ é…ç½®æ¨¡æ¿

### å¿«é€Ÿæµ‹è¯•é…ç½®
```yaml
max_samples: 10
num_train_epochs: 1.0
save_steps: 2
logging_steps: 1
```

### ç”Ÿäº§ç¯å¢ƒé…ç½®
```yaml
max_samples: 10000
num_train_epochs: 5.0
save_steps: 1000
logging_steps: 100
```

### é«˜æ€§èƒ½é…ç½®
```yaml
lora_rank: 16
lora_alpha: 32
learning_rate: 5.0e-4
per_device_train_batch_size: 2
```
